{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGlEz2VVOWGG"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "id": "GVFPJLIdf3mu",
        "outputId": "0e586335-48b3-4a84-934b-b2a3e20ca064"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1408506528.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    132\u001b[0m   )\n\u001b[1;32m    133\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip dataset\n",
        "import zipfile\n",
        "with zipfile.ZipFile(\"/content/drive/MyDrive/dataset_oxe.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"/content\")\n",
        "\n",
        "# Set dataset path\n",
        "DATASET_BASE_PATH = \"/content/oxe-colab/training_data\"\n",
        "\n",
        "print(f\"‚úÖ Dataset ready at: {DATASET_BASE_PATH}\")"
      ],
      "metadata": {
        "id": "lJkzTKG1hJtP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "\n",
        "# Login to WandB\n",
        "print(\"üîë Login to Weights & Biases\")\n",
        "print(\"Get your API key from: https://wandb.ai/authorize\")\n",
        "\n",
        "wandb.login()\n",
        "\n",
        "# Verify login\n",
        "print(\"\\n‚úÖ WandB login successful!\")\n",
        "\n",
        "# Project configuration\n",
        "WANDB_PROJECT = \"qwen2-vl-behaviortree\"\n",
        "WANDB_RUN_NAME = \"qwen2-8b-bt-finetune-v1\"  # Change for each run\n",
        "WANDB_NOTES = \"Fine-tuning Qwen2-VL 8B on BehaviorTree dataset with LoRA\""
      ],
      "metadata": {
        "id": "9CP9HlojmsB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PglJeZZoOWGG"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os, re\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    import torch; v = re.match(r\"[0-9]{1,}\\.[0-9]{1,}\", str(torch.__version__)).group(0)\n",
        "    xformers = \"xformers==\" + (\"0.0.33.post1\" if v==\"2.9\" else \"0.0.32.post2\" if v==\"2.8\" else \"0.0.29.post3\")\n",
        "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets==4.3.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "!pip install transformers==4.57.1\n",
        "!pip install --no-deps trl==0.22.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xLDGk41C7IF"
      },
      "source": [
        "### Unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmUBVEnvCDJv"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastVisionModel # FastLanguageModel for LLMs\n",
        "import torch\n",
        "\n",
        "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
        "fourbit_models = [\n",
        "    \"unsloth/Llama-3.2-11B-Vision-Instruct-bnb-4bit\", # Llama 3.2 vision support\n",
        "    \"unsloth/Llama-3.2-11B-Vision-bnb-4bit\",\n",
        "    \"unsloth/Llama-3.2-90B-Vision-Instruct-bnb-4bit\", # Can fit in a 80GB card!\n",
        "    \"unsloth/Llama-3.2-90B-Vision-bnb-4bit\",\n",
        "\n",
        "    \"unsloth/Pixtral-12B-2409-bnb-4bit\",              # Pixtral fits in 16GB!\n",
        "    \"unsloth/Pixtral-12B-Base-2409-bnb-4bit\",         # Pixtral base model\n",
        "\n",
        "    \"unsloth/Qwen2-VL-2B-Instruct-bnb-4bit\",          # Qwen2 VL support\n",
        "    \"unsloth/Qwen2-VL-7B-Instruct-bnb-4bit\",\n",
        "    \"unsloth/Qwen2-VL-72B-Instruct-bnb-4bit\",\n",
        "\n",
        "    \"unsloth/llava-v1.6-mistral-7b-hf-bnb-4bit\",      # Any Llava variant works!\n",
        "    \"unsloth/llava-1.5-7b-hf-bnb-4bit\",\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastVisionModel.from_pretrained(\n",
        "    \"unsloth/Qwen3-VL-8B-Instruct-unsloth-bnb-4bit\",\n",
        "    load_in_4bit = True, # Use 4bit to reduce memory use. False for 16bit LoRA.\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for long context\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd9bTZd1aaL"
      },
      "source": [
        "We now add LoRA adapters for parameter efficient finetuning - this allows us to only efficiently train 1% of all parameters.\n",
        "\n",
        "**[NEW]** We also support finetuning ONLY the vision part of the model, or ONLY the language part. Or you can select both! You can also select to finetune the attention or the MLP layers!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bZsfBuZDeCL"
      },
      "outputs": [],
      "source": [
        "model = FastVisionModel.get_peft_model(\n",
        "    model,\n",
        "    finetune_vision_layers     = True, # False if not finetuning vision layers\n",
        "    finetune_language_layers   = True, # False if not finetuning language layers\n",
        "    finetune_attention_modules = True, # False if not finetuning attention layers\n",
        "    finetune_mlp_modules       = True, # False if not finetuning MLP layers\n",
        "\n",
        "    r = 16,           # The larger, the higher the accuracy, but might overfit\n",
        "    lora_alpha = 16,  # Recommended alpha == r at least\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        "    # target_modules = \"all-linear\", # Optional now! Can specify a list if needed\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vITh0KVJ10qX"
      },
      "source": [
        "<a name=\"Data\"></a>\n",
        "### Data Prep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjY75GoYUCB8"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "os.chdir('/content')\n",
        "\n",
        "# Load dataset\n",
        "train_dataset_raw = load_dataset(\"json\", data_files=\"dataset_oxe/train/data.jsonl\", split=\"train\")\n",
        "val_dataset_raw = load_dataset(\"json\", data_files=\"dataset_oxe/val/data.jsonl\", split=\"train\")\n",
        "\n",
        "# ========================================\n",
        "# FIX PER QWEN3-VL: Convert format\n",
        "# ========================================\n",
        "def convert_for_qwen3(example, base_path):\n",
        "    \"\"\"\n",
        "    Qwen3-VL richiede formato specifico per image placeholder\n",
        "    \"\"\"\n",
        "    # Load image\n",
        "    img_path = os.path.join(base_path, example[\"messages\"][0][\"content\"][1][\"image\"])\n",
        "    image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "    # Get texts\n",
        "    user_text = example[\"messages\"][0][\"content\"][0][\"text\"]\n",
        "    assistant_text = example[\"messages\"][1][\"content\"][0][\"text\"]\n",
        "\n",
        "    # ‚úÖ FIX: Qwen3 usa formato diverso per content\n",
        "    new_example = {\n",
        "        \"messages\": [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"image\", \"image\": image},  # ‚Üê Image PRIMA\n",
        "                    {\"type\": \"text\", \"text\": user_text}  # ‚Üê Text DOPO\n",
        "                ]\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"text\", \"text\": assistant_text}\n",
        "                ]\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    return new_example\n",
        "\n",
        "# Convert to list format (no Arrow serialization issues)\n",
        "print(\"Converting dataset for Qwen3-VL...\")\n",
        "train_dataset = []\n",
        "for example in train_dataset_raw:\n",
        "    converted = convert_for_qwen3(example, \"/content/dataset_oxe/train\")\n",
        "    train_dataset.append(converted)\n",
        "\n",
        "val_dataset = []\n",
        "for example in val_dataset_raw:\n",
        "    converted = convert_for_qwen3(example, \"/content/dataset_oxe/val\")\n",
        "    val_dataset.append(converted)\n",
        "\n",
        "print(f\"‚úÖ Dataset converted!\")\n",
        "print(f\"   Train: {len(train_dataset)} samples\")\n",
        "print(f\"   Val: {len(val_dataset)} samples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Show first sample\n",
        "print(\"=\"*60)\n",
        "print(\"FIRST TRAINING SAMPLE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "sample = train_dataset[0]\n",
        "messages = sample[\"messages\"]\n",
        "\n",
        "# Show user message\n",
        "user_msg = messages[0]\n",
        "print(\"\\nüìù USER MESSAGE:\")\n",
        "for content in user_msg[\"content\"]:\n",
        "    if content[\"type\"] == \"text\":\n",
        "        print(f\"\\nText:\\n{content['text'][:300]}...\")  # First 300 chars\n",
        "    elif content[\"type\"] == \"image\":\n",
        "        print(f\"\\nImage: {type(content['image'])} - Size: {content['image'].size}\")\n",
        "\n",
        "# Show assistant message\n",
        "assistant_msg = messages[1]\n",
        "print(\"\\nü§ñ ASSISTANT MESSAGE (Target BT):\")\n",
        "bt_xml = assistant_msg[\"content\"][0][\"text\"]\n",
        "print(f\"\\n{bt_xml[:500]}...\")  # First 500 chars\n",
        "\n",
        "# Display image\n",
        "print(\"\\nüñºÔ∏è FRAME IMAGE:\")\n",
        "display(user_msg[\"content\"][1][\"image\"].resize((400, 300)))\n"
      ],
      "metadata": {
        "id": "1qvSy43FhsCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9CBpiISFa6C"
      },
      "source": [
        "To format the dataset, all vision finetuning tasks should be formatted as follows:\n",
        "\n",
        "```python\n",
        "[\n",
        "{ \"role\": \"user\",\n",
        "  \"content\": [{\"type\": \"text\",  \"text\": Q}, {\"type\": \"image\", \"image\": image} ]\n",
        "},\n",
        "{ \"role\": \"assistant\",\n",
        "  \"content\": [{\"type\": \"text\",  \"text\": A} ]\n",
        "},\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FecKS-dA82f5"
      },
      "source": [
        "Let's first see before we do any finetuning what the model outputs for the first example!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vcat4UxA81vr"
      },
      "outputs": [],
      "source": [
        "FastVisionModel.for_inference(model)\n",
        "\n",
        "# Take sample\n",
        "sample = train_dataset[2]\n",
        "image = sample[\"messages\"][0][\"content\"][0][\"image\"]\n",
        "user_text = sample[\"messages\"][0][\"content\"][1][\"text\"]\n",
        "ground_truth = sample[\"messages\"][1][\"content\"][0][\"text\"]\n",
        "\n",
        "# Preview\n",
        "print(\"Image:\")\n",
        "display(image)\n",
        "print(f\"\\nInstruction: {[l for l in user_text.split('\\n') if 'INSTRUCTION:' in l][0]}\")\n",
        "print(f\"\\nGround Truth:\\n{ground_truth}\\n\")\n",
        "\n",
        "# Inference\n",
        "messages = [{\"role\": \"user\", \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": user_text}]}]\n",
        "input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
        "inputs = tokenizer(image, input_text, add_special_tokens=False, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
        "print(\"Prediction:\")\n",
        "_ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=512, use_cache=True, temperature=1.5, min_p=0.1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "Now let's train our model. We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. We also support TRL's `DPOTrainer`!\n",
        "\n",
        "We use our new `UnslothVisionDataCollator` which will help in our vision finetuning setup."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# INITIALIZE WANDB RUN\n",
        "# ========================================\n",
        "import wandb\n",
        "\n",
        "# Project configuration\n",
        "WANDB_PROJECT = \"qwen2-vl-behaviortree\"\n",
        "WANDB_RUN_NAME = \"qwen2-8b-bt-finetune-v1\"  # Change for each run\n",
        "WANDB_NOTES = \"Fine-tuning Qwen2-VL 8B on BehaviorTree dataset with LoRA\"\n",
        "\n",
        "# Initialize run\n",
        "wandb.init(\n",
        "    project=WANDB_PROJECT,\n",
        "    name=WANDB_RUN_NAME,\n",
        "    notes=WANDB_NOTES,\n",
        "    config={\n",
        "        # Model\n",
        "        \"model_name\": \"unsloth/Qwen2-VL-8B-Instruct-bnb-4bit\",\n",
        "        \"quantization\": \"4bit\",\n",
        "\n",
        "        # LoRA config\n",
        "        \"lora_r\": 16,\n",
        "        \"lora_alpha\": 16,\n",
        "        \"lora_dropout\": 0,\n",
        "        \"target_modules\": \"all-linear\",\n",
        "\n",
        "        # Training hyperparameters\n",
        "        \"num_epochs\": 3,\n",
        "        \"max_steps\": 30,\n",
        "\n",
        "        \"batch_size\": 2,\n",
        "        \"gradient_accumulation_steps\": 4,\n",
        "        \"learning_rate\": 2e-4,\n",
        "        \"warmup_steps\": 10,\n",
        "        \"optimizer\": \"adamw_8bit\",\n",
        "        \"weight_decay\": 0.01,\n",
        "        \"lr_scheduler\": \"linear\",\n",
        "\n",
        "        # Dataset\n",
        "        \"train_samples\": len(train_dataset),\n",
        "        \"val_samples\": len(val_dataset),\n",
        "        \"max_seq_length\": 2048,\n",
        "\n",
        "        # Save strategy\n",
        "        \"save_steps\": 100,\n",
        "        \"eval_steps\": 100,\n",
        "    }\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ WandB run initialized: {WANDB_PROJECT}/{WANDB_RUN_NAME}\")\n",
        "print(f\"üìä View at: {wandb.run.get_url()}\")\n"
      ],
      "metadata": {
        "id": "gyuKocinm5zQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95_Nn-89DhsL"
      },
      "outputs": [],
      "source": [
        "from unsloth.trainer import UnslothVisionDataCollator\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "FastVisionModel.for_training(model) # Enable for training!\n",
        "\n",
        "# ========================================\n",
        "# IMPORTANT: Save checkpoints to Google Drive!\n",
        "# ========================================\n",
        "DRIVE_OUTPUT_DIR = \"/content/drive/MyDrive/qwen2_vl_bt_training_outputs\"\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    data_collator = UnslothVisionDataCollator(model, tokenizer), # Must use!\n",
        "    train_dataset = train_dataset,\n",
        "    eval_dataset = val_dataset,  # ADD VALIDATION!\n",
        "    args = SFTConfig(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 10,\n",
        "\n",
        "        # Training duration\n",
        "        # num_train_epochs = 3,  # 3 epochs for ~1500 samples\n",
        "        max_steps = 20,\n",
        "\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not torch.cuda.is_bf16_supported(),\n",
        "        bf16 = torch.cuda.is_bf16_supported(),\n",
        "        logging_steps = 10,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "\n",
        "        # ========================================\n",
        "        # CHECKPOINT SETTINGS (SAVE TO DRIVE!)\n",
        "        # ========================================\n",
        "        output_dir = DRIVE_OUTPUT_DIR,  # ‚Üê Save to Drive!\n",
        "\n",
        "        save_strategy = \"steps\",        # Save every N steps\n",
        "        save_steps = 100,               # Save every 100 steps\n",
        "        save_total_limit = 3,           # Keep only last 3 checkpoints\n",
        "\n",
        "        # Evaluation\n",
        "        eval_strategy = \"steps\",\n",
        "        eval_steps = 100,               # Evaluate every 100 steps\n",
        "        load_best_model_at_end = True,\n",
        "        metric_for_best_model = \"eval_loss\",\n",
        "\n",
        "        # Report\n",
        "        # WANDB INTEGRATION\n",
        "        # ========================================\n",
        "        report_to = \"wandb\",  # ‚Üê Changed from \"none\"!\n",
        "        run_name = WANDB_RUN_NAME,\n",
        "        logging_first_step = True,\n",
        "\n",
        "\n",
        "        # You MUST put the below items for vision finetuning:\n",
        "        remove_unused_columns = False,\n",
        "        dataset_text_field = \"\",\n",
        "        dataset_kwargs = {\"skip_prepare_dataset\": True},\n",
        "        dataset_num_proc = 4,\n",
        "        max_seq_length = 2048,\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(f\"üìÅ Checkpoints will be saved to: {DRIVE_OUTPUT_DIR}\")\n",
        "print(f\"üíæ Auto-save every 100 steps, keeping last 3 checkpoints\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2ejIt2xSNKKp"
      },
      "outputs": [],
      "source": [
        "# @title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqxqAZ7KJ4oL"
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCqnaKmlO1U9"
      },
      "outputs": [],
      "source": [
        "# @title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(\n",
        "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
        ")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# LOG TRAINING SUMMARY AND FINISH WANDB\n",
        "# ========================================\n",
        "# Calculate final metrics\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "\n",
        "# Training time\n",
        "training_time_seconds = trainer_stats.metrics['train_runtime']\n",
        "training_time_minutes = round(training_time_seconds / 60, 2)\n",
        "training_time_hours = round(training_time_minutes / 60, 2)\n",
        "\n",
        "# Log summary metrics\n",
        "wandb.summary[\"final_train_loss\"] = trainer_stats.metrics.get('train_loss', 'N/A')\n",
        "wandb.summary[\"final_eval_loss\"] = trainer_stats.metrics.get('eval_loss', 'N/A')\n",
        "wandb.summary[\"training_time_minutes\"] = training_time_minutes\n",
        "wandb.summary[\"training_time_hours\"] = training_time_hours\n",
        "wandb.summary[\"peak_memory_gb\"] = used_memory\n",
        "wandb.summary[\"peak_memory_percent\"] = used_percentage\n",
        "wandb.summary[\"lora_memory_gb\"] = used_memory_for_lora\n",
        "wandb.summary[\"lora_memory_percent\"] = lora_percentage\n",
        "\n",
        "# Print summary\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìä TRAINING SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"‚è±Ô∏è  Training time: {training_time_minutes} min ({training_time_hours} hours)\")\n",
        "print(f\"üìâ Final train loss: {trainer_stats.metrics.get('train_loss', 'N/A')}\")\n",
        "print(f\"üìâ Final eval loss: {trainer_stats.metrics.get('eval_loss', 'N/A')}\")\n",
        "print(f\"üíæ Peak memory: {used_memory} GB ({used_percentage}%)\")\n",
        "print(f\"üíæ LoRA memory: {used_memory_for_lora} GB ({lora_percentage}%)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Save artifact to WandB (LoRA weights)\n",
        "print(\"\\nüì¶ Saving LoRA weights as WandB artifact...\")\n",
        "artifact = wandb.Artifact(\n",
        "    name=f\"qwen2-vl-bt-lora-{wandb.run.id}\",\n",
        "    type=\"model\",\n",
        "    description=\"Fine-tuned Qwen2-VL 8B LoRA adapters for BehaviorTree generation\"\n",
        ")\n",
        "artifact.add_dir(\"/content/drive/MyDrive/qwen2_vl_8b_bt_lora_FINAL\")\n",
        "wandb.log_artifact(artifact)\n",
        "\n",
        "print(f\"‚úÖ Artifact saved to WandB!\")\n",
        "\n",
        "# Finish WandB run\n",
        "wandb.finish()\n",
        "\n",
        "print(f\"\\nüéâ Training complete! View full report at: {wandb.run.get_url()}\")\n"
      ],
      "metadata": {
        "id": "5GZmHhnLnaP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from unsloth import FastVisionModel\n",
        "# from unsloth.trainer import UnslothVisionDataCollator\n",
        "# from trl import SFTTrainer, SFTConfig\n",
        "# import torch\n",
        "\n",
        "# # ========================================\n",
        "# # 1. LOAD BASE MODEL\n",
        "# # ========================================\n",
        "# print(\"Loading base model...\")\n",
        "# model, tokenizer = FastVisionModel.from_pretrained(\n",
        "#     \"unsloth/Qwen2-VL-8B-Instruct-bnb-4bit\",\n",
        "#     load_in_4bit=True,\n",
        "#     use_gradient_checkpointing=\"unsloth\",\n",
        "# )\n",
        "\n",
        "# # ========================================\n",
        "# # 2. SETUP LORA (same as initial training)\n",
        "# # ========================================\n",
        "# print(\"Setting up LoRA...\")\n",
        "# model = FastVisionModel.get_peft_model(\n",
        "#     model,\n",
        "#     finetune_vision_layers=True,\n",
        "#     finetune_language_layers=True,\n",
        "#     finetune_attention_modules=True,\n",
        "#     finetune_mlp_modules=True,\n",
        "#     r=16,\n",
        "#     lora_alpha=16,\n",
        "#     lora_dropout=0,\n",
        "#     bias=\"none\",\n",
        "#     random_state=3407,\n",
        "#     use_rslora=False,\n",
        "#     loftq_config=None,\n",
        "#     target_modules=\"all-linear\",\n",
        "# )\n",
        "\n",
        "# FastVisionModel.for_training(model)\n",
        "\n",
        "# # ========================================\n",
        "# # 3. CREATE TRAINER (same config)\n",
        "# # ========================================\n",
        "# DRIVE_OUTPUT_DIR = \"/content/drive/MyDrive/qwen2_vl_bt_training_outputs\"\n",
        "\n",
        "# trainer = SFTTrainer(\n",
        "#     model = model,\n",
        "#     tokenizer = tokenizer,\n",
        "#     data_collator = UnslothVisionDataCollator(model, tokenizer),\n",
        "#     train_dataset = train_dataset,\n",
        "#     eval_dataset = val_dataset,\n",
        "#     args = SFTConfig(\n",
        "#         per_device_train_batch_size = 2,\n",
        "#         gradient_accumulation_steps = 4,\n",
        "#         warmup_steps = 10,\n",
        "\n",
        "#         # Training duration\n",
        "#         num_train_epochs = 3,\n",
        "\n",
        "#         learning_rate = 2e-4,\n",
        "#         fp16 = not torch.cuda.is_bf16_supported(),\n",
        "#         bf16 = torch.cuda.is_bf16_supported(),\n",
        "#         logging_steps = 10,\n",
        "#         optim = \"adamw_8bit\",\n",
        "#         weight_decay = 0.01,\n",
        "#         lr_scheduler_type = \"linear\",\n",
        "#         seed = 3407,\n",
        "\n",
        "#         # Checkpoint settings\n",
        "#         output_dir = DRIVE_OUTPUT_DIR,\n",
        "#         save_strategy = \"steps\",\n",
        "#         save_steps = 100,\n",
        "#         save_total_limit = 3,\n",
        "\n",
        "#         # Evaluation\n",
        "#         eval_strategy = \"steps\",\n",
        "#         eval_steps = 100,\n",
        "#         load_best_model_at_end = True,\n",
        "#         metric_for_best_model = \"eval_loss\",\n",
        "\n",
        "#         # Report\n",
        "#         # WANDB INTEGRATION\n",
        "          # ========================================\n",
        "#         report_to = \"wandb\",\n",
        "#         run_name = WANDB_RUN_NAME,\n",
        "#         logging_first_step = True,\n",
        "\n",
        "#         # Vision finetuning requirements\n",
        "#         remove_unused_columns = False,\n",
        "#         dataset_text_field = \"\",\n",
        "#         dataset_kwargs = {\"skip_prepare_dataset\": True},\n",
        "#         dataset_num_proc = 4,\n",
        "#         max_seq_length = 2048,\n",
        "#     ),\n",
        "# )\n",
        "\n",
        "# # ========================================\n",
        "# # 4. RESUME FROM LATEST CHECKPOINT\n",
        "# # ========================================\n",
        "# import os\n",
        "\n",
        "# # Check if checkpoints exist\n",
        "# checkpoint_dirs = [d for d in os.listdir(DRIVE_OUTPUT_DIR)\n",
        "#                    if d.startswith(\"checkpoint-\")] if os.path.exists(DRIVE_OUTPUT_DIR) else []\n",
        "\n",
        "# if checkpoint_dirs:\n",
        "#     latest_checkpoint = max(checkpoint_dirs, key=lambda x: int(x.split(\"-\")[1]))\n",
        "#     checkpoint_path = os.path.join(DRIVE_OUTPUT_DIR, latest_checkpoint)\n",
        "#     print(f\"\\nüîÑ Resuming from checkpoint: {checkpoint_path}\")\n",
        "#     trainer_stats = trainer.train(resume_from_checkpoint=checkpoint_path)\n",
        "# else:\n",
        "#     print(\"\\nüÜï No checkpoint found. Starting fresh training...\")\n",
        "#     trainer_stats = trainer.train()\n",
        "\n",
        "# print(\"\\n‚úÖ Training completed!\")\n"
      ],
      "metadata": {
        "id": "_2e5Y4Fpl5th"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Let's run the model! You can change the instruction and input - leave the output blank!\n",
        "\n",
        "We use `min_p = 0.1` and `temperature = 1.5`. Read this [Tweet](https://x.com/menhguin/status/1826132708508213629) for more information on why."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kR3gIAX-SM2q"
      },
      "outputs": [],
      "source": [
        "FastVisionModel.for_inference(model)\n",
        "\n",
        "# Take sample\n",
        "sample = val_dataset[5]\n",
        "image = sample[\"messages\"][0][\"content\"][0][\"image\"]\n",
        "user_text = sample[\"messages\"][0][\"content\"][1][\"text\"]\n",
        "ground_truth = sample[\"messages\"][1][\"content\"][0][\"text\"]\n",
        "\n",
        "# Preview\n",
        "print(\"Image:\")\n",
        "display(image)\n",
        "print(f\"\\nInstruction: {[l for l in user_text.split('\\n') if 'INSTRUCTION:' in l][0]}\")\n",
        "print(f\"\\nGround Truth:\\n{ground_truth}\\n\")\n",
        "\n",
        "# Inference\n",
        "messages = [{\"role\": \"user\", \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": user_text}]}]\n",
        "input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
        "inputs = tokenizer(image, input_text, add_special_tokens=False, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
        "print(\"Prediction:\")\n",
        "_ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=512, use_cache=True, temperature=1.5, min_p=0.1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upcOlWe7A1vc"
      },
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# SAVE LORA ADAPTERS TO GOOGLE DRIVE\n",
        "# ========================================\n",
        "# This is the MOST IMPORTANT save - small size, reloadable\n",
        "\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Ensure drive is mounted\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# Save path on Google Drive\n",
        "DRIVE_SAVE_PATH = \"/content/drive/MyDrive/qwen2_vl_8b_bt_lora\"\n",
        "\n",
        "# Save LoRA adapters (SMALL - only ~100MB!)\n",
        "print(\"Saving LoRA adapters to Google Drive...\")\n",
        "model.save_pretrained(DRIVE_SAVE_PATH)\n",
        "tokenizer.save_pretrained(DRIVE_SAVE_PATH)\n",
        "\n",
        "print(f\"‚úÖ LoRA adapters saved to: {DRIVE_SAVE_PATH}\")\n",
        "print(\"üì¶ This is your MAIN checkpoint - use this to resume training or inference!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# TEST: RELOAD LORA FROM DRIVE\n",
        "# ========================================\n",
        "# This tests that your save worked correctly\n",
        "\n",
        "TEST_RELOAD = False  # Set to True to test reload\n",
        "\n",
        "if TEST_RELOAD:\n",
        "    from unsloth import FastVisionModel\n",
        "\n",
        "    DRIVE_SAVE_PATH = \"/content/drive/MyDrive/qwen2_vl_8b_bt_lora\"\n",
        "\n",
        "    print(f\"Reloading model from: {DRIVE_SAVE_PATH}\")\n",
        "\n",
        "    # Load base model + LoRA\n",
        "    model_reloaded, tokenizer_reloaded = FastVisionModel.from_pretrained(\n",
        "        model_name=\"unsloth/Qwen2-VL-8B-Instruct-bnb-4bit\",  # Base model\n",
        "        load_in_4bit=True,\n",
        "    )\n",
        "\n",
        "    # Load LoRA weights\n",
        "    from peft import PeftModel\n",
        "    model_reloaded = PeftModel.from_pretrained(model_reloaded, DRIVE_SAVE_PATH)\n",
        "\n",
        "    print(\"‚úÖ Model reloaded successfully!\")\n",
        "    print(\"üß™ Test inference:\")\n",
        "\n",
        "    # Quick test\n",
        "    FastVisionModel.for_inference(model_reloaded)\n",
        "    sample = val_dataset[0]\n",
        "    image = sample[\"messages\"][0][\"content\"][1][\"image\"]\n",
        "    user_text = sample[\"messages\"][0][\"content\"][0][\"text\"]\n",
        "\n",
        "    messages = [{\"role\": \"user\", \"content\": [\n",
        "        {\"type\": \"image\"},\n",
        "        {\"type\": \"text\", \"text\": user_text}\n",
        "    ]}]\n",
        "\n",
        "    input_text = tokenizer_reloaded.apply_chat_template(messages, add_generation_prompt=True)\n",
        "    inputs = tokenizer_reloaded(image, input_text, add_special_tokens=False, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    from transformers import TextStreamer\n",
        "    text_streamer = TextStreamer(tokenizer_reloaded, skip_prompt=True)\n",
        "    _ = model_reloaded.generate(**inputs, streamer=text_streamer, max_new_tokens=256, use_cache=True)\n",
        "else:\n",
        "    print(\"‚è≠Ô∏è Skipped reload test (set TEST_RELOAD=True to test)\")\n"
      ],
      "metadata": {
        "id": "bY7IanvllDVk"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}