{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMFXCLD37P4Yp+IiGhmSmnG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cristianobattistini/oxe-bt-pipeline/blob/refactor-vlm-modular/nb/evalutation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation: BLEU and ROUGE\n"
      ],
      "metadata": {
        "id": "_5aMjFU_KQCv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import and download libraries"
      ],
      "metadata": {
        "id": "rahcVLfJKPWF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "import re, torch\n",
        "\n",
        "# Rileva versione di torch per scegliere xformers compatibile (come nel tuo codice unsloth)\n",
        "v = re.match(r\"[0-9]{1,}\\.[0-9]{1,}\", str(torch.__version__)).group(0)\n",
        "if v == \"2.9\":\n",
        "    xformers = \"xformers==0.0.33.post1\"\n",
        "elif v == \"2.8\":\n",
        "    xformers = \"xformers==0.0.32.post2\"\n",
        "else:\n",
        "    xformers = \"xformers==0.0.29.post3\"\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# Stack base per UNSLOTH (stesso schema che hai usato nei training)\n",
        "# --------------------------------------------------------------------\n",
        "!pip install --no-deps bitsandbytes accelerate {xformers} peft trl==0.22.2 triton cut_cross_entropy unsloth_zoo evaluate rouge_score\n",
        "!pip install sentencepiece protobuf \"datasets==4.3.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "!pip install --no-deps unsloth\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# Extra richiesti da SmolVLM2 (in aggiunta allo stack sopra)\n",
        "# --------------------------------------------------------------------\n",
        "# (accelerate, datasets, peft, bitsandbytes sono giÃ  installati sopra)\n",
        "!pip install tensorboard av num2words sentence-transformers\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# Versione di transformers comune a tutti i modelli\n",
        "#   - compatibile con requirement SmolVLM2: \">=4.41.0,<5.0.0\"\n",
        "#   - giÃ  usata con unsloth nei tuoi notebook (4.56.2)\n",
        "# --------------------------------------------------------------------\n",
        "!pip install \"transformers==4.56.2\"\n"
      ],
      "metadata": {
        "id": "QsZTM9OqOIeD"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9c6a6b65",
        "outputId": "df28547c-e5bc-471b-a1b0-e2c06480f157"
      },
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "print(\"Attempting login via Colab Secrets...\")\n",
        "\n",
        "try:\n",
        "    # Recupera il token dai segreti di Colab (assicurati che la chiave si chiami 'HF_TOKEN')\n",
        "    hf_token = userdata.get('HF_TOKEN2')\n",
        "    if hf_token:\n",
        "        login(token=hf_token)\n",
        "        print(\"Login successful!\")\n",
        "    else:\n",
        "        print(\"Secret 'HF_TOKEN' not found or empty.\")\n",
        "except Exception as e:\n",
        "    print(f\"Login failed: {e}\")\n",
        "    print(\"Make sure you have added a secret named 'HF_TOKEN' in the Colab Secrets tab (key icon on the left).\")\n",
        "\n",
        "# Una volta fatto il login, riesegui la cella di generazione (quella sopra che inizia con 'Retry mask-conditioned synthesis')."
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting login via Colab Secrets...\n",
            "Login successful!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "from google.colab import drive\n",
        "import os, zipfile\n",
        "\n",
        "from datasets import load_dataset\n",
        "from PIL import Image\n",
        "\n",
        "import numpy as np\n",
        "import json\n",
        "from pathlib import Path\n",
        "from torch.utils.data import Subset\n",
        "import io\n",
        "import base64\n",
        "from datasets import load_dataset\n",
        "from PIL import Image\n",
        "import os\n",
        "from google.colab import userdata\n",
        "from openai import AzureOpenAI\n",
        "import re, gc, torch, json\n",
        "from pathlib import Path\n",
        "import evaluate\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Mount Drive\n",
        "# ------------------------------------------------------------------\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Global paths (persist on Drive)\n",
        "# ------------------------------------------------------------------\n",
        "THESIS_ROOT = \"/content/drive/MyDrive/thesis\"\n",
        "EVAL_ROOT   = os.path.join(THESIS_ROOT, \"eval_bt_metrics\")\n",
        "Path(EVAL_ROOT).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "TIMESTAMP = datetime.now().strftime(\"_%d%m%Y_%H%M\")\n",
        "print(\"TIMESTAMP:\", TIMESTAMP)\n",
        "print(\"THESIS_ROOT:\", THESIS_ROOT)\n",
        "print(\"EVAL_ROOT:  \", EVAL_ROOT)\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# LoRA paths for each model (adjust folder names if needed)\n",
        "# ------------------------------------------------------------------\n",
        "LORA_PATHS = {\n",
        "    \"gemma\": os.path.join(\n",
        "        THESIS_ROOT, \"gemma\", \"gemma3_4b_vision_bt_lora\"\n",
        "    ),\n",
        "    \"qwen2dot5\": os.path.join(\n",
        "        THESIS_ROOT, \"qwen2dot5\", \"qwen2dot5-3B-Instruct_bt_lora\"\n",
        "    ),\n",
        "    \"qwen3\": os.path.join(\n",
        "        THESIS_ROOT, \"qwen3\", \"qwen3_vl_8b_bt_lora\"\n",
        "    ),\n",
        "    \"smolvlm2\": os.path.join(\n",
        "        THESIS_ROOT, \"smolvlm2\", \"lora_adapter\"\n",
        "    ),\n",
        "}\n",
        "\n",
        "for name, path in LORA_PATHS.items():\n",
        "    print(name, \"â†’\", path, \"   exists:\", os.path.isdir(path))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpfaudxPKjwg",
        "outputId": "c37ad2f1-57ec-4b52-bbbd-9fc6b061d033"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "TIMESTAMP: _12122025_1040\n",
            "THESIS_ROOT: /content/drive/MyDrive/thesis\n",
            "EVAL_ROOT:   /content/drive/MyDrive/thesis/eval_bt_metrics\n",
            "gemma â†’ /content/drive/MyDrive/thesis/gemma/gemma3_4b_vision_bt_lora    exists: True\n",
            "qwen2dot5 â†’ /content/drive/MyDrive/thesis/qwen2dot5/qwen2dot5-3B-Instruct_bt_lora    exists: True\n",
            "qwen3 â†’ /content/drive/MyDrive/thesis/qwen3/qwen3_vl_8b_bt_lora    exists: True\n",
            "smolvlm2 â†’ /content/drive/MyDrive/thesis/smolvlm2/lora_adapter    exists: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Dataset"
      ],
      "metadata": {
        "id": "AQ9bb_btK5yX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('/content')\n",
        "\n",
        "# Unzip dataset_oxe.zip (adjust path if needed)\n",
        "zip_path = \"/content/drive/MyDrive/dataset_oxe.zip\"\n",
        "if os.path.exists(zip_path):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(\"/content\")\n",
        "    print(\"âœ… Unzipped dataset_oxe.zip into /content\")\n",
        "else:\n",
        "    print(\"âš ï¸ dataset_oxe.zip not found at\", zip_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtvM_s04K7nI",
        "outputId": "82258e63-9dec-461f-9674-036d42f08452"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Unzipped dataset_oxe.zip into /content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('/content')\n",
        "\n",
        "# Load dataset\n",
        "train_dataset_raw = load_dataset(\"json\", data_files=\"dataset_oxe/train/data.jsonl\", split=\"train\")\n",
        "val_dataset_raw = load_dataset(\"json\", data_files=\"dataset_oxe/val/data.jsonl\", split=\"train\")\n",
        "\n",
        "# ========================================\n",
        "# FIX PER QWEN3-VL: Convert format\n",
        "# ========================================\n",
        "def convert_positions_text_image(example, base_path):\n",
        "    \"\"\"\n",
        "    Qwen3-VL richiede formato specifico per image placeholder\n",
        "    \"\"\"\n",
        "    # Load image\n",
        "    img_path = os.path.join(base_path, example[\"messages\"][0][\"content\"][1][\"image\"])\n",
        "    image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "    # Get texts\n",
        "    user_text = example[\"messages\"][0][\"content\"][0][\"text\"]\n",
        "    assistant_text = example[\"messages\"][1][\"content\"][0][\"text\"]\n",
        "\n",
        "    new_example = {\n",
        "        \"messages\": [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"image\", \"image\": image},  # â† Image PRIMA\n",
        "                    {\"type\": \"text\", \"text\": user_text}  # â† Text DOPO\n",
        "                ]\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"text\", \"text\": assistant_text}\n",
        "                ]\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    return new_example\n",
        "\n",
        "# Convert to list format (no Arrow serialization issues)\n",
        "print(\"Converting dataset for Qwen3-VL...\")\n",
        "train_dataset = []\n",
        "for example in train_dataset_raw:\n",
        "    converted = convert_positions_text_image(example, \"/content/dataset_oxe/train\")\n",
        "    train_dataset.append(converted)\n",
        "\n",
        "val_dataset = []\n",
        "for example in val_dataset_raw:\n",
        "    converted = convert_positions_text_image(example, \"/content/dataset_oxe/val\")\n",
        "    val_dataset.append(converted)\n",
        "\n",
        "print(f\"âœ… Dataset ready! Train: {len(train_dataset)}, Val: {len(val_dataset)}\")\n"
      ],
      "metadata": {
        "id": "-_Hj2i9KcLOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class MessagesDataset(Dataset):\n",
        "    def __init__(self, list_examples):\n",
        "        self.data = list_examples\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "train_ds = MessagesDataset(train_dataset)\n",
        "eval_ds  = MessagesDataset(val_dataset)\n",
        "\n",
        "print(\"train_ds size:\", len(train_ds))\n",
        "print(\"eval_ds size:\", len(eval_ds))\n"
      ],
      "metadata": {
        "id": "NqnJ-7FhLFlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SUBSET_SIZE = 50\n",
        "INDICES_PATH = Path(EVAL_ROOT) / \"eval_indices_50.json\"\n",
        "\n",
        "if INDICES_PATH.exists():\n",
        "    # Reload the same indices used in previous runs\n",
        "    with open(INDICES_PATH, \"r\") as f:\n",
        "        subset_indices = json.load(f)\n",
        "    print(f\"âœ… Loaded {len(subset_indices)} indices from {INDICES_PATH}\")\n",
        "else:\n",
        "    # Generate once a random but deterministic subset\n",
        "    rng = np.random.RandomState(42)  # fixed seed\n",
        "    all_indices = np.arange(len(eval_ds))\n",
        "    subset_indices = rng.choice(all_indices, size=SUBSET_SIZE, replace=False)\n",
        "    subset_indices = sorted(subset_indices.tolist())\n",
        "\n",
        "    with open(INDICES_PATH, \"w\") as f:\n",
        "        json.dump(subset_indices, f)\n",
        "    print(f\"âœ… Generated and saved {len(subset_indices)} indices to {INDICES_PATH}\")\n",
        "\n",
        "# Create the PyTorch subset dataset\n",
        "eval_subset_ds = Subset(eval_ds, subset_indices)\n",
        "\n",
        "print(\"eval_ds size:\", len(eval_ds))\n",
        "print(\"eval_subset_ds size:\", len(eval_subset_ds))\n",
        "print(\"First 10 indices used for subset:\", subset_indices[:10])\n"
      ],
      "metadata": {
        "id": "EM7_xlSTMG5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LOADER"
      ],
      "metadata": {
        "id": "lVpvLcNEXJVZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# AZURE OPENAI CLIENT (GPT-5)\n",
        "# ========================================\n",
        "\n",
        "\n",
        "print(\"ðŸ”‘ Setup Azure OpenAI client (GPT-5)\")\n",
        "\n",
        "azure_openai_key         = userdata.get(\"azure_openai_key\")\n",
        "azure_openai_endpoint    = userdata.get(\"azure_openai_endpoint\")\n",
        "azure_openai_api_version = userdata.get(\"azure_openai_api_version\")\n",
        "azure_openai_region      = userdata.get(\"azure_openai_region\")  # opzionale\n",
        "\n",
        "if not azure_openai_key or not azure_openai_endpoint or not azure_openai_api_version:\n",
        "    raise RuntimeError(\n",
        "        \"Missing one of: azure_openai_key, azure_openai_endpoint, azure_openai_api_version \"\n",
        "        \"in Colab userdata secrets.\"\n",
        "    )\n",
        "\n",
        "client_azure = AzureOpenAI(\n",
        "    api_key        = azure_openai_key,\n",
        "    azure_endpoint = azure_openai_endpoint,\n",
        "    api_version    = azure_openai_api_version,\n",
        ")\n",
        "\n",
        "print(\"âœ… Azure OpenAI client initialised!\")\n",
        "\n",
        "# Nomi dei deployment per i due modelli (adattali ai tuoi deployment Azure)\n",
        "GPT5_INSTANT_DEPLOYMENT  = \"gpt-5.1-mini\"      # es: deployment per GPT-5 instant\n",
        "GPT5_THINKING_DEPLOYMENT = \"gpt-5.1-thinking\"  # es: deployment per GPT-5 thinking\n",
        "\n",
        "# Prompt di sistema opzionale (puoi arricchirlo con le stesse istruzioni del training)\n",
        "GPT_SYSTEM_PROMPT = (\n",
        "    \"You are an assistant that generates BehaviorTree.CPP XML trees for robot tasks. \"\n",
        "    \"Given a textual description of an episode, you must output ONLY the XML for the behavior tree.\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "MutWwWEEbXBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastVisionModel, get_chat_template\n",
        "from peft import PeftModel\n",
        "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
        "\n",
        "# Base model ids (adjust if needed)\n",
        "GEMMA_BASE_ID    = \"unsloth/gemma-3-4b-pt\"\n",
        "QWEN25_BASE_ID   = \"unsloth/Qwen2.5-VL-3B-Instruct\"\n",
        "QWEN3_BASE_ID    = \"unsloth/Qwen3-VL-8B-Instruct-unsloth-bnb-4bit\"\n",
        "SMOLVLM2_BASE_ID = \"HuggingFaceTB/SmolVLM2-2.2B-Instruct\"\n",
        "\n",
        "\n",
        "def get_tokenizer_from_processor(processor):\n",
        "    \"\"\"\n",
        "    Alcuni processor hanno .tokenizer, altri lo sono direttamente.\n",
        "    Restituisce sempre un oggetto con .decode(...).\n",
        "    \"\"\"\n",
        "    return getattr(processor, \"tokenizer\", processor)\n",
        "\n",
        "\n",
        "def make_cleanup_fn(objects_to_free):\n",
        "    def cleanup():\n",
        "        for obj in objects_to_free:\n",
        "            try:\n",
        "                del obj\n",
        "            except Exception:\n",
        "                pass\n",
        "        gc.collect()\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "    return cleanup\n",
        "\n",
        "def make_vision_inference_fn(model, processor, device, use_text_images: bool):\n",
        "    \"\"\"\n",
        "    use_text_images = False  â†’ unsloth: processor(image, input_text, ...)\n",
        "    use_text_images = True   â†’ SmolVLM2: processor(text=input_text, images=image, ...)\n",
        "    \"\"\"\n",
        "    tokenizer = get_tokenizer_from_processor(processor)\n",
        "\n",
        "    def inference_fn(image, user_text):\n",
        "        # 1) Messages identici per tutti i modelli\n",
        "        messages = [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"image\", \"image\": image},\n",
        "                    {\"type\": \"text\", \"text\": user_text},\n",
        "                ],\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        # 2) Applica il chat template del processor\n",
        "        input_text = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
        "\n",
        "        # 3) Costruisci gli inputs con la firma corretta\n",
        "        if use_text_images:\n",
        "            # SmolVLM2-style\n",
        "            inputs = processor(\n",
        "                text   = input_text,\n",
        "                images = image,\n",
        "                add_special_tokens=False,\n",
        "                return_tensors=\"pt\",\n",
        "            ).to(device)\n",
        "        else:\n",
        "            # unsloth FastVisionModel-style\n",
        "            inputs = processor(\n",
        "                image,\n",
        "                input_text,\n",
        "                add_special_tokens=False,\n",
        "                return_tensors=\"pt\",\n",
        "            ).to(device)\n",
        "\n",
        "        # 4) Generazione deterministica\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens = 512,\n",
        "                do_sample      = False,\n",
        "                temperature    = 0.0,\n",
        "                use_cache      = True,\n",
        "            )\n",
        "\n",
        "        # 5) Rimuovi il prompt dagli output prima del decode\n",
        "        input_len = inputs[\"input_ids\"].shape[-1]\n",
        "        gen_ids = outputs[0, input_len:]\n",
        "        text = tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
        "        return text\n",
        "\n",
        "    return inference_fn\n",
        "\n",
        "\n",
        "import time\n",
        "\n",
        "def azure_chat_with_retry(\n",
        "    client,\n",
        "    model: str,\n",
        "    messages,\n",
        "    max_tokens: int = 512,\n",
        "    temperature: float = 0.0,\n",
        "    max_retries: int = 3,\n",
        "    base_delay: float = 2.0,\n",
        "):\n",
        "    \"\"\"\n",
        "    Chiamata robusta all'API Azure OpenAI con un semplice meccanismo di retry.\n",
        "\n",
        "    - Riprova fino a max_retries volte in caso di eccezioni.\n",
        "    - Attende base_delay * (tentativo+1) secondi tra un tentativo e l'altro.\n",
        "    - In caso di fallimento definitivo, restituisce una stringa sentinella\n",
        "      per non interrompere l'evaluation.\n",
        "    \"\"\"\n",
        "    last_exc = None\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            resp = client.chat.completions.create(\n",
        "                model       = model,\n",
        "                messages    = messages,\n",
        "                max_tokens  = max_tokens,\n",
        "                temperature = temperature,\n",
        "            )\n",
        "            return resp.choices[0].message.content\n",
        "        except Exception as e:\n",
        "            last_exc = e\n",
        "            wait_time = base_delay * (attempt + 1)\n",
        "            print(\n",
        "                f\"[Azure GPT] Error on attempt {attempt+1}/{max_retries} \"\n",
        "                f\"for model '{model}': {e}. Retrying in {wait_time:.1f}s...\"\n",
        "            )\n",
        "            time.sleep(wait_time)\n",
        "\n",
        "    # Se siamo qui, tutti i tentativi sono falliti.\n",
        "    # Opzione A (piÃ¹ robusta per la pipeline): restituisci un testo sentinella\n",
        "    # in modo che l'evaluation continui.\n",
        "    print(\n",
        "        f\"[Azure GPT] All {max_retries} attempts failed for model '{model}'. \"\n",
        "        \"Returning sentinel prediction.\"\n",
        "    )\n",
        "    return \"[AZURE_ERROR] Prediction unavailable due to repeated API errors.\"\n",
        "\n",
        "    # Opzione B (piÃ¹ severa): rilancia l'eccezione per fermare tutto.\n",
        "    # raise last_exc\n",
        "\n",
        "\n",
        "def load_model(model_key: str):\n",
        "    \"\"\"\n",
        "    model_key âˆˆ {\"gemma\", \"qwen2dot5\", \"qwen3\", \"smolvlm2\"}\n",
        "\n",
        "    Returns:\n",
        "      inference_fn(image, user_text) -> str\n",
        "      cleanup_fn() -> None\n",
        "    \"\"\"\n",
        "    model_key = model_key.lower()\n",
        "\n",
        "    # --------------------------- GEMMA ---------------------------\n",
        "    if model_key == \"gemma\":\n",
        "        lora_dir = LORA_PATHS[\"gemma\"]\n",
        "        print(f\"[gemma] Loading base model + LoRA from: {lora_dir}\")\n",
        "\n",
        "        model, processor = FastVisionModel.from_pretrained(\n",
        "            model_name   = GEMMA_BASE_ID,\n",
        "            load_in_4bit = True,\n",
        "        )\n",
        "\n",
        "        # In training avevi usato get_chat_template(\"gemma-3\")\n",
        "        processor = get_chat_template(processor, \"gemma-3\")\n",
        "\n",
        "        model = PeftModel.from_pretrained(model, lora_dir)\n",
        "        FastVisionModel.for_inference(model)\n",
        "        model.eval()\n",
        "\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        inference_fn = make_vision_inference_fn(\n",
        "            model,\n",
        "            processor,\n",
        "            device,\n",
        "            use_text_images = False,   # unsloth-style: processor(image, text, ...)\n",
        "        )\n",
        "        cleanup_fn = make_cleanup_fn([model, processor])\n",
        "        return inference_fn, cleanup_fn\n",
        "\n",
        "    # ------------------------ QWEN 2.5 ---------------------------\n",
        "    if model_key == \"qwen2dot5\":\n",
        "        lora_dir = LORA_PATHS[\"qwen2dot5\"]\n",
        "        print(f\"[qwen2dot5] Loading base model + LoRA from: {lora_dir}\")\n",
        "\n",
        "        model, processor = FastVisionModel.from_pretrained(\n",
        "            model_name   = QWEN25_BASE_ID,\n",
        "            load_in_4bit = True,\n",
        "        )\n",
        "\n",
        "        model = PeftModel.from_pretrained(model, lora_dir)\n",
        "        FastVisionModel.for_inference(model)\n",
        "        model.eval()\n",
        "\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        inference_fn = make_vision_inference_fn(\n",
        "            model,\n",
        "            processor,\n",
        "            device,\n",
        "            use_text_images = False,   # come Gemma, usi processor(image, text, ...)\n",
        "        )\n",
        "        cleanup_fn = make_cleanup_fn([model, processor])\n",
        "        return inference_fn, cleanup_fn\n",
        "\n",
        "    # -------------------------- QWEN 3 ---------------------------\n",
        "    if model_key == \"qwen3\":\n",
        "        lora_dir = LORA_PATHS[\"qwen3\"]\n",
        "        print(f\"[qwen3] Loading base model + LoRA from: {lora_dir}\")\n",
        "\n",
        "        model, processor = FastVisionModel.from_pretrained(\n",
        "            model_name   = QWEN3_BASE_ID,\n",
        "            load_in_4bit = True,\n",
        "        )\n",
        "\n",
        "        model = PeftModel.from_pretrained(model, lora_dir)\n",
        "        FastVisionModel.for_inference(model)\n",
        "        model.eval()\n",
        "\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        inference_fn = make_vision_inference_fn(\n",
        "            model,\n",
        "            processor,\n",
        "            device,\n",
        "            use_text_images = False,    # Qwen3 usa processor(text=..., images=...)\n",
        "        )\n",
        "        cleanup_fn = make_cleanup_fn([model, processor])\n",
        "        return inference_fn, cleanup_fn\n",
        "\n",
        "    # ------------------------ SMOLVLM2 ---------------------------\n",
        "    if model_key == \"smolvlm2\":\n",
        "        lora_dir = LORA_PATHS[\"smolvlm2\"]\n",
        "        print(f\"[smolvlm2] Loading base model + LoRA from: {lora_dir}\")\n",
        "\n",
        "        processor = AutoProcessor.from_pretrained(lora_dir)\n",
        "\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        model = AutoModelForImageTextToText.from_pretrained(\n",
        "            SMOLVLM2_BASE_ID,\n",
        "            torch_dtype          = torch.float16,   # allinea a fp16\n",
        "            attn_implementation  = \"eager\",         # evita SDPA integrata che rompe sui dtype misti\n",
        "        ).to(device)\n",
        "\n",
        "        model = PeftModel.from_pretrained(model, lora_dir)\n",
        "        model.eval()\n",
        "\n",
        "        inference_fn = make_vision_inference_fn(\n",
        "            model,\n",
        "            processor,\n",
        "            device,\n",
        "            use_text_images = True,    # text=..., images=...\n",
        "        )\n",
        "        cleanup_fn = make_cleanup_fn([model, processor])\n",
        "        return inference_fn, cleanup_fn\n",
        "\n",
        "    # --------------------- GPT-5 INSTANT ------------------------\n",
        "    if model_key == \"gpt5_instant\":\n",
        "        print(\"[gpt5_instant] Using Azure OpenAI GPT-5 Instant deployment:\",\n",
        "              GPT5_INSTANT_DEPLOYMENT)\n",
        "\n",
        "        def inference_fn(image, user_text):\n",
        "            \"\"\"\n",
        "            Usa esattamente lo stesso user_text degli altri modelli\n",
        "            e la stessa immagine, codificata in base64.\n",
        "            \"\"\"\n",
        "            # Codifica l'immagine PIL in base64 (JPEG)\n",
        "            buf = io.BytesIO()\n",
        "            image.save(buf, format=\"JPEG\")\n",
        "            img_b64 = base64.b64encode(buf.getvalue()).decode(\"utf-8\")\n",
        "\n",
        "            # Messaggi: stesso prompt testuale, + immagine come image_url\n",
        "            messages = [\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": [\n",
        "                        {\"type\": \"text\", \"text\": user_text},\n",
        "                        {\n",
        "                            \"type\": \"image_url\",\n",
        "                            \"image_url\": {\n",
        "                                \"url\": f\"data:image/jpeg;base64,{img_b64}\"\n",
        "                            },\n",
        "                        },\n",
        "                    ],\n",
        "                }\n",
        "            ]\n",
        "\n",
        "            # Chiamata con retry\n",
        "            text = azure_chat_with_retry(\n",
        "                client      = client_azure,\n",
        "                model       = GPT5_INSTANT_DEPLOYMENT,\n",
        "                messages    = messages,\n",
        "                max_tokens  = 512,\n",
        "                temperature = 0.0,\n",
        "                max_retries = 3,    # puoi aumentare/diminuire\n",
        "                base_delay  = 2.0,  # backoff 2s, 4s, 6s\n",
        "            )\n",
        "            return text\n",
        "\n",
        "        # Nessuna GPU locale da liberare\n",
        "        def cleanup_fn():\n",
        "            return\n",
        "\n",
        "        return inference_fn, cleanup_fn\n",
        "\n",
        "\n",
        "    # -------------------- GPT-5 THINKING ------------------------\n",
        "    if model_key == \"gpt5_thinking\":\n",
        "        print(\"[gpt5_thinking] Using Azure OpenAI GPT-5 Thinking deployment:\",\n",
        "              GPT5_THINKING_DEPLOYMENT)\n",
        "\n",
        "        def inference_fn(image, user_text):\n",
        "            # Codifica l'immagine PIL in base64 (JPEG)\n",
        "            buf = io.BytesIO()\n",
        "            image.save(buf, format=\"JPEG\")\n",
        "            img_b64 = base64.b64encode(buf.getvalue()).decode(\"utf-8\")\n",
        "\n",
        "            messages = [\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": [\n",
        "                        {\"type\": \"text\", \"text\": user_text},\n",
        "                        {\n",
        "                            \"type\": \"image_url\",\n",
        "                            \"image_url\": {\n",
        "                                \"url\": f\"data:image/jpeg;base64,{img_b64}\"\n",
        "                            },\n",
        "                        },\n",
        "                    ],\n",
        "                }\n",
        "            ]\n",
        "\n",
        "            text = azure_chat_with_retry(\n",
        "                client      = client_azure,\n",
        "                model       = GPT5_THINKING_DEPLOYMENT,\n",
        "                messages    = messages,\n",
        "                max_tokens  = 512,\n",
        "                temperature = 0.0,\n",
        "                max_retries = 3,\n",
        "                base_delay  = 2.0,\n",
        "            )\n",
        "            return text\n",
        "\n",
        "        def cleanup_fn():\n",
        "            return\n",
        "\n",
        "        return inference_fn, cleanup_fn\n",
        "\n"
      ],
      "metadata": {
        "id": "mbrfraZbXK6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EVALUATION METHODS"
      ],
      "metadata": {
        "id": "VK_qaUIvOAdi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------------\n",
        "#  Helper: extract (image, user_text, ground_truth)\n",
        "# -------------------------------------------------------\n",
        "def extract_fields(sample):\n",
        "    \"\"\"\n",
        "    Given a sample with the 'messages' structure, returns:\n",
        "      image       : PIL image\n",
        "      user_text   : full user prompt string (with INSTRUCTION, etc.)\n",
        "      ground_truth: BT ground truth (assistant text)\n",
        "    \"\"\"\n",
        "    image = sample[\"messages\"][0][\"content\"][0][\"image\"]\n",
        "    user_text = sample[\"messages\"][0][\"content\"][1][\"text\"]\n",
        "    ground_truth = sample[\"messages\"][1][\"content\"][0][\"text\"]\n",
        "    return image, user_text, ground_truth\n",
        "\n",
        "\n",
        "# -------------------------------------------------------\n",
        "#  Helper: simple normalization of BT text\n",
        "#          (minimize formatting noise for ROUGE/BLEU)\n",
        "# -------------------------------------------------------\n",
        "_WHITESPACE_RE = re.compile(r\"\\s+\")\n",
        "\n",
        "def normalize_bt(text: str) -> str:\n",
        "    if text is None:\n",
        "        return \"\"\n",
        "    # Strip ends and collapse all whitespace (spaces / newlines / tabs) to single spaces\n",
        "    return _WHITESPACE_RE.sub(\" \", text.strip())\n",
        "\n",
        "\n",
        "# -------------------------------------------------------\n",
        "#  Metrics (loaded once, reused for all models)\n",
        "# -------------------------------------------------------\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "bleu  = evaluate.load(\"bleu\")\n",
        "print(\"Loaded metrics: ROUGE + BLEU\")\n"
      ],
      "metadata": {
        "id": "OHCdeniLOALQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import json\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "def evaluate_model(model_key, eval_indices, dataset, force=False):\n",
        "    \"\"\"\n",
        "    Esegue inference sul subset e calcola ROUGE/BLEU per un singolo modello.\n",
        "\n",
        "    Args:\n",
        "        model_key    : \"gemma\", \"qwen2dot5\", \"qwen3\", \"smolvlm2\",\n",
        "                       \"gpt5_instant\", \"gpt5_thinking\"\n",
        "        eval_indices : lista di indici nel dataset (subset_indices)\n",
        "        dataset      : es. eval_ds\n",
        "        force        : se False e metrics.json esiste, non ricalcola\n",
        "\n",
        "    Output:\n",
        "        dict con metriche aggregate.\n",
        "        Salva anche:\n",
        "          - EVAL_ROOT/<model_key>/metrics.json\n",
        "          - EVAL_ROOT/<model_key>/predictions.jsonl\n",
        "    \"\"\"\n",
        "    model_key = model_key.lower()\n",
        "\n",
        "    model_dir = Path(EVAL_ROOT) / model_key\n",
        "    model_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    metrics_path = model_dir / \"metrics.json\"\n",
        "    preds_path   = model_dir / \"predictions.jsonl\"\n",
        "\n",
        "    # Se abbiamo giÃ  tutto e force=False, non rifacciamo il lavoro\n",
        "    if metrics_path.exists() and preds_path.exists() and not force:\n",
        "        print(f\"[{model_key}] metrics already found, skipping (force=False).\")\n",
        "        with open(metrics_path, \"r\") as f:\n",
        "            metrics = json.load(f)\n",
        "        return metrics\n",
        "\n",
        "    # 1) Carica modello + inference_fn + cleanup_fn\n",
        "    print(f\"\\n===== Evaluating model: {model_key} =====\")\n",
        "    print(f\"Subset size: {len(eval_indices)} examples\")\n",
        "    inference_fn, cleanup_fn = load_model(model_key)\n",
        "\n",
        "    predictions        = []  # normalized predictions (per metriche)\n",
        "    references         = []  # normalized references (per metriche)\n",
        "    raw_predictions    = []  # raw text output (per salvataggio)\n",
        "    raw_references     = []  # reference originale (se ti serve in futuro)\n",
        "\n",
        "    # 2) Loop sul subset con tqdm\n",
        "    for step, idx in enumerate(tqdm(eval_indices, desc=f\"{model_key} eval\", unit=\"ex\")):\n",
        "        sample = dataset[idx]\n",
        "        image, user_text, ground_truth = extract_fields(sample)\n",
        "\n",
        "        # Predizione raw dal modello\n",
        "        pred_raw = inference_fn(image, user_text)\n",
        "\n",
        "        # Normalizza prediction + reference per le metriche\n",
        "        pred_norm = normalize_bt(pred_raw)\n",
        "        ref_norm  = normalize_bt(ground_truth)\n",
        "\n",
        "        raw_predictions.append(pred_raw)\n",
        "        raw_references.append(ground_truth)\n",
        "        predictions.append(pred_norm)\n",
        "        references.append(ref_norm)\n",
        "\n",
        "    # 3) Calcolo metriche aggregate\n",
        "    rouge_res = rouge.compute(\n",
        "        predictions = predictions,\n",
        "        references  = references,\n",
        "        use_stemmer = True,\n",
        "    )\n",
        "    # BLEU di HF evaluate si aspetta references come lista di liste\n",
        "    bleu_res = bleu.compute(\n",
        "        predictions = predictions,\n",
        "        references  = [[r] for r in references],\n",
        "    )\n",
        "\n",
        "    metrics = {\n",
        "        \"model\":        model_key,\n",
        "        \"num_examples\": len(eval_indices),\n",
        "        \"rouge1\":       rouge_res[\"rouge1\"]    * 100,\n",
        "        \"rouge2\":       rouge_res[\"rouge2\"]    * 100,\n",
        "        \"rougeL\":       rouge_res[\"rougeL\"]    * 100,\n",
        "        \"rougeLsum\":    rouge_res[\"rougeLsum\"] * 100,\n",
        "        \"bleu\":         bleu_res[\"bleu\"]       * 100,\n",
        "    }\n",
        "\n",
        "    print(\"\\n--- Aggregated metrics ---\")\n",
        "    for k in [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\", \"bleu\"]:\n",
        "        print(f\"{k}: {metrics[k]:.2f}\")\n",
        "\n",
        "    # 4) Salva metrics + predizioni su Drive\n",
        "    with open(metrics_path, \"w\") as f:\n",
        "        json.dump(metrics, f, indent=2)\n",
        "\n",
        "    with open(preds_path, \"w\") as f:\n",
        "        for idx, pred_raw, ref_raw, ref_norm in zip(\n",
        "            eval_indices, raw_predictions, raw_references, references\n",
        "        ):\n",
        "            rec = {\n",
        "                \"idx\":         int(idx),\n",
        "                \"prediction\":  pred_raw,\n",
        "                \"reference\":   ref_raw,\n",
        "                \"reference_n\": ref_norm,\n",
        "            }\n",
        "            f.write(json.dumps(rec) + \"\\n\")\n",
        "\n",
        "    print(f\"\\n[{model_key}] Saved metrics to     {metrics_path}\")\n",
        "    print(f\"[{model_key}] Saved predictions to {preds_path}\")\n",
        "\n",
        "    # 5) Cleanup GPU / memoria\n",
        "    cleanup_fn()\n",
        "\n",
        "    return metrics\n"
      ],
      "metadata": {
        "id": "6DVH-n3deBt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GEMMA"
      ],
      "metadata": {
        "id": "w4FSvxRRNgr_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# EVALUATION â€“ GEMMA\n",
        "# ========================================\n",
        "\n",
        "MODEL_KEY = \"gemma\"\n",
        "print(f\"\\n Starting evaluation for: {MODEL_KEY}\")\n",
        "\n",
        "metrics_gemma = evaluate_model(\n",
        "    model_key    = MODEL_KEY,\n",
        "    eval_indices = subset_indices,\n",
        "    dataset      = eval_ds,\n",
        "    force        = False,  # metti True se vuoi ricalcolare anche se esistono giÃ  i file\n",
        ")\n",
        "\n",
        "print(\"\\n=== GEMMA â€“ METRICS ===\")\n",
        "for k, v in metrics_gemma.items():\n",
        "    print(f\"{k}: {v}\")\n"
      ],
      "metadata": {
        "id": "C3HgqRC3NjKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## QWEN 2.5"
      ],
      "metadata": {
        "id": "bIrfG5_bNkgF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# EVALUATION â€“ QWEN2.5\n",
        "# ========================================\n",
        "\n",
        "MODEL_KEY = \"qwen2dot5\"\n",
        "print(f\"\\n Starting evaluation for: {MODEL_KEY}\")\n",
        "\n",
        "metrics_qwen2dot5 = evaluate_model(\n",
        "    model_key    = MODEL_KEY,\n",
        "    eval_indices = subset_indices,\n",
        "    dataset      = eval_ds,\n",
        "    force        = False,\n",
        ")\n",
        "\n",
        "print(\"\\n=== QWEN2.5 â€“ METRICS ===\")\n",
        "for k, v in metrics_qwen2dot5.items():\n",
        "    print(f\"{k}: {v}\")\n"
      ],
      "metadata": {
        "id": "4Osh2SU6NpIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## QWEN 3"
      ],
      "metadata": {
        "id": "DgOsypIMNp3g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# EVALUATION â€“ QWEN3\n",
        "# ========================================\n",
        "\n",
        "MODEL_KEY = \"qwen3\"\n",
        "print(f\"\\n Starting evaluation for: {MODEL_KEY}\")\n",
        "\n",
        "metrics_qwen3 = evaluate_model(\n",
        "    model_key    = MODEL_KEY,\n",
        "    eval_indices = subset_indices,\n",
        "    dataset      = eval_ds,\n",
        "    force        = False,\n",
        ")\n",
        "\n",
        "print(\"\\n=== QWEN3 â€“ METRICS ===\")\n",
        "for k, v in metrics_qwen3.items():\n",
        "    print(f\"{k}: {v}\")\n"
      ],
      "metadata": {
        "id": "U_Mf3yuXNtdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SMOLVLM2"
      ],
      "metadata": {
        "id": "s9-SdPyQNubJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# EVALUATION â€“ SMOLVLM2\n",
        "# ========================================\n",
        "\n",
        "MODEL_KEY = \"smolvlm2\"\n",
        "print(f\"\\n Starting evaluation for: {MODEL_KEY}\")\n",
        "\n",
        "metrics_smolvlm2 = evaluate_model(\n",
        "    model_key    = MODEL_KEY,\n",
        "    eval_indices = subset_indices,\n",
        "    dataset      = eval_ds,\n",
        "    force        = False,\n",
        ")\n",
        "\n",
        "print(\"\\n=== SMOLVLM2 â€“ METRICS ===\")\n",
        "for k, v in metrics_smolvlm2.items():\n",
        "    print(f\"{k}: {v}\")\n"
      ],
      "metadata": {
        "id": "gA9-IXInNwms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPT-5 INSTANT"
      ],
      "metadata": {
        "id": "o0mcFBWvNzCt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# EVALUATION â€“ GPT-5 INSTANT\n",
        "# ========================================\n",
        "# Richiede che il client Azure (client_azure, GPT5_INSTANT_DEPLOYMENT, ecc.)\n",
        "# sia giÃ  inizializzato correttamente.\n",
        "\n",
        "MODEL_KEY = \"gpt5_instant\"\n",
        "print(f\"\\n Starting evaluation for: {MODEL_KEY}\")\n",
        "\n",
        "metrics_gpt5_instant = evaluate_model(\n",
        "    model_key    = MODEL_KEY,\n",
        "    eval_indices = subset_indices,\n",
        "    dataset      = eval_ds,\n",
        "    force        = False,\n",
        ")\n",
        "\n",
        "print(\"\\n=== GPT-5 INSTANT â€“ METRICS ===\")\n",
        "for k, v in metrics_gpt5_instant.items():\n",
        "    print(f\"{k}: {v}\")\n"
      ],
      "metadata": {
        "id": "PaXPiRuZN2zc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPT-5"
      ],
      "metadata": {
        "id": "9hoHGvPLN3yO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# EVALUATION â€“ GPT-5 THINKING\n",
        "# ========================================\n",
        "# Anche qui serve il client Azure giÃ  inizializzato.\n",
        "\n",
        "MODEL_KEY = \"gpt5_thinking\"\n",
        "print(f\"\\n Starting evaluation for: {MODEL_KEY}\")\n",
        "\n",
        "metrics_gpt5_thinking = evaluate_model(\n",
        "    model_key    = MODEL_KEY,\n",
        "    eval_indices = subset_indices,\n",
        "    dataset      = eval_ds,\n",
        "    force        = False,\n",
        ")\n",
        "\n",
        "print(\"\\n=== GPT-5 THINKING â€“ METRICS ===\")\n",
        "for k, v in metrics_gpt5_thinking.items():\n",
        "    print(f\"{k}: {v}\")\n"
      ],
      "metadata": {
        "id": "eysrT9iTN5pi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FINAL EVALUATION"
      ],
      "metadata": {
        "id": "-wA26UskN6pW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import json\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "EVAL_ROOT = Path(EVAL_ROOT)\n",
        "\n",
        "DISPLAY_NAMES = {\n",
        "    \"gemma\":        \"Gemma-3 4B-Vision\",\n",
        "    \"qwen2dot5\":    \"Qwen2.5-VL-3B\",\n",
        "    \"qwen3\":        \"Qwen3-VL-8B\",\n",
        "    \"smolvlm2\":     \"SmolVLM2-2.2B\",\n",
        "    \"gpt5_instant\": \"GPT-5 Instant\",\n",
        "    \"gpt5_thinking\":\"GPT-5 Thinking\",\n",
        "}\n",
        "\n",
        "rows = []\n",
        "\n",
        "print(f\"Scanning evaluation directory: {EVAL_ROOT}\")\n",
        "\n",
        "for subdir in sorted(EVAL_ROOT.iterdir()):\n",
        "    if not subdir.is_dir():\n",
        "        continue\n",
        "\n",
        "    metrics_file = subdir / \"metrics.json\"\n",
        "    if not metrics_file.exists():\n",
        "        print(f\"No metrics.json in {subdir}, skipping.\")\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        with open(metrics_file, \"r\") as f:\n",
        "            metrics = json.load(f)\n",
        "\n",
        "        model_key    = subdir.name\n",
        "        display_name = DISPLAY_NAMES.get(model_key, model_key)\n",
        "\n",
        "        row = {\n",
        "            \"model_key\":  model_key,\n",
        "            \"model\":      display_name,\n",
        "            \"rouge1\":     metrics.get(\"rouge1\", float(\"nan\")),\n",
        "            \"rouge2\":     metrics.get(\"rouge2\", float(\"nan\")),\n",
        "            \"rougeL\":     metrics.get(\"rougeL\", float(\"nan\")),\n",
        "            \"rougeLsum\":  metrics.get(\"rougeLsum\", float(\"nan\")),\n",
        "            \"bleu\":       metrics.get(\"bleu\", float(\"nan\")),\n",
        "        }\n",
        "        rows.append(row)\n",
        "        print(f\"Loaded metrics for model: {display_name}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {metrics_file}: {e}\")\n",
        "\n",
        "if not rows:\n",
        "    print(\"No metrics found under EVAL_ROOT. Run the per-model evaluations first.\")\n",
        "else:\n",
        "    # -----------------------------\n",
        "    # DataFrame base\n",
        "    # -----------------------------\n",
        "    df_raw = pd.DataFrame(rows)\n",
        "\n",
        "    # Ordine dei modelli in legenda\n",
        "    model_order = [\n",
        "        DISPLAY_NAMES.get(k, k)\n",
        "        for k in DISPLAY_NAMES.keys()\n",
        "        if (df_raw[\"model_key\"] == k).any()\n",
        "    ]\n",
        "    if model_order:\n",
        "        df_raw[\"model\"] = pd.Categorical(\n",
        "            df_raw[\"model\"],\n",
        "            categories=model_order,\n",
        "            ordered=True,\n",
        "        )\n",
        "        df_raw = df_raw.sort_values(\"model\")\n",
        "\n",
        "    metric_order = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\", \"bleu\"]\n",
        "\n",
        "    # index = metrica, colonne = modello\n",
        "    df_plot = (\n",
        "        df_raw\n",
        "        .set_index(\"model\")[metric_order]\n",
        "        .T\n",
        "    )\n",
        "\n",
        "    # -----------------------------\n",
        "    # Tabella riepilogativa (se vuoi vedere i numeri)\n",
        "    # -----------------------------\n",
        "    # display(df_plot)\n",
        "\n",
        "    # -----------------------------\n",
        "    # Bar plot\n",
        "    # -----------------------------\n",
        "    fig, ax = plt.subplots(figsize=(14, 4))  # piÃ¹ largo e piÃ¹ alto\n",
        "\n",
        "    colors = [\n",
        "        \"#004c6d\",  # Gemma\n",
        "        \"#2f6bba\",  # Qwen2.5\n",
        "        \"#7b3c8c\",  # Qwen3\n",
        "        \"#d75452\",  # SmolVLM2\n",
        "        \"#f39c35\",  # GPT-5 Instant\n",
        "        \"#7f8c8d\",  # GPT-5 Thinking\n",
        "    ][:len(df_plot.columns)]\n",
        "\n",
        "    df_plot.plot(\n",
        "        kind=\"bar\",\n",
        "        ax=ax,\n",
        "        width=0.60,        # barre piÃ¹ larghe\n",
        "        color=colors,\n",
        "    )\n",
        "\n",
        "    ax.set_ylabel(\"Score (%)\", fontsize=11)\n",
        "    ax.set_xlabel(\"\")\n",
        "    ax.set_xticklabels(\n",
        "        [\"ROUGE 1\", \"ROUGE 2\", \"ROUGE L\", \"ROUGE Lsum\", \"BLEU\"],\n",
        "        rotation=0,\n",
        "        fontsize=10,\n",
        "    )\n",
        "    ax.set_ylim(0, 100)\n",
        "    ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
        "\n",
        "    # Legenda sopra il grafico, orizzontale\n",
        "    ax.legend(\n",
        "        title=\"\",\n",
        "        loc=\"lower center\",\n",
        "        bbox_to_anchor=(0.5, 1.05),\n",
        "        ncol=len(df_plot.columns),\n",
        "        frameon=False,\n",
        "        fontsize=10,\n",
        "    )\n",
        "\n",
        "    ax.spines[\"top\"].set_visible(False)\n",
        "    ax.spines[\"right\"].set_visible(False)\n",
        "\n",
        "    # Valori sopra ogni barra (font piÃ¹ grande)\n",
        "    try:\n",
        "        for container in ax.containers:\n",
        "            ax.bar_label(container, fmt=\"%.0f\", padding=3, fontsize=9)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "VeCGe0YHN9dR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}